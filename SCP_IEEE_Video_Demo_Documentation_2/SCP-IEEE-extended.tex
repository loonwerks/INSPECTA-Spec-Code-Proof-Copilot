\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{todonotes}
% -------------------- Algorithms (boxed style, IEEE-friendly) --------------------
\usepackage[ruled,vlined]{algorithm2e}
\DontPrintSemicolon
\SetKwInput{KwIn}{Input}
\SetKwInput{KwOut}{Output}
\SetKw{KwTo}{to}
\SetKw{KwOr}{or}
%-----

% Setup for listings style
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=tb,
    captionpos=b,
    backgroundcolor=\color{gray!5},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    showstringspaces=false
}

\begin{document}

\title{Bridging Natural Language to Certified
Implementation: A Neuro-Symbolic High-Assurance
MBSE Pipeline in SysML v2}

\author{\IEEEauthorblockN{Amer Tahat, David Hardin, Isaac Amundson, and Darren Cofer}
\IEEEauthorblockA{\textit{Collins Aerospace, An RTX Business} \\
%\textit{DARPA PROVERS Program}\\
\{amer.tahat, david.hardin, isaac.amundson, darren.cofer\}@collins.com}
}

\maketitle

\begin{abstract}
The development of high-confidence avionics systems demands rigorous traceability from requirements to implementation, yet manual allocation often severs the link between high-level assurance cases and low-level code. While Generative AI (GenAI) offers a potential solution for automation, its deployment in safety-critical workflows is severely constrained by hallucinations, prohibitive token costs, and two critical barriers: the scarcity of open-source domain data and the restriction on fine-tuning state-of-the-art proprietary models (e.g., GPT-5 class) for open use.

In this paper, we introduce a novel neuro-symbolic pipeline designed to bridge this gap. Our approach integrates state-of-the-art Codex agents into the DARPA PROVERS INSPECTA toolchain, a workflow that orchestrates the automated translation of SysML v2 system models into high-assurance code. By leveraging the HAMR code generator and Logika verifier, the pipeline ensures that properties established in the architectural model are mathematically preserved in the final implementation.

To circumvent fine-tuning limitations, we introduce a novel \textit{Meta-Rule} methodology that functions as a verifiable surrogate for black-box weight adaptation. Unlike transient prompting strategies, this framework \textit{crystallizes} ephemeral In-Context Learning (ICL) into persistent, expert-curated abstraction patterns. This allows the system to iteratively "adapt" its formal translation logic and drive a self-healing generate–verify–repair loop (translating English to GUMBO contracts and HAMR code), where HAMR/Logika feedback triggers largely automated, Meta-Rule guided revisions with minimal user intervention until system model integration and code-level verification succeed, effectively bypassing the data scarcity and proprietary restrictions that currently prohibit fine-tuning state-of-the-art models. 

We evaluate this methodology on the Isolette Infant Incubator, utilized here as a standard proxy for safety-critical subsystems. From natural language requirements, our operational copilot achieved 100\% formal code-level verification for the application logic in 25 minutes using $\sim$4 million tokens. In contrast, the baseline failed to produce verifiable artifacts despite consuming nearly 19 million tokens while processing extensive system documentation ($>$37 pages). The Meta-Rule–guided approach demonstrated a 58$\times$ increase in task throughput, reconciling the efficiency of GenAI with the strict rigor required for certifiable critical system software. We conclude by discussing current limitations and outlining future directions for scaling this architecture to broader cyber-physical applications.
\end{abstract}

\begin{IEEEkeywords}
SWE agents, Neuro-symbolic AI, Formal Methods, SysML v2, MBSE, Supervised Self-Healing, Supervised Self-Adaptation.
\end{IEEEkeywords}

\section{Introduction}

The development of high-confidence digital avionics systems faces a fundamental tension between increasing complexity and the rigid demands of certification. A primary challenge in this domain is ensuring rigorous traceability from high-level natural language requirements to low-level implementation. In traditional Model-Based Systems Engineering (MBSE) workflows, the manual allocation of requirements often severs the traceability link between the architectural assurance case and the software code. Without strong model-to-implementation synchronization, critical design flaws are often discovered late in integration or after fielding, driving up costs and jeopardizing safety certification~\cite{hardin2025dasc}.

Generative AI (GenAI), and especially Large Language Models (LLMs), appear well-suited to automate parts of this transition by translating requirements into structured specifications, contracts, and implementation scaffolding. However, directly deploying LLMs in safety-critical workflows remains problematic. Beyond stochastic hallucinations and the practical burden of token-heavy prompting, two structural barriers limit adoption in aerospace environments: (i) scarce open-source, domain-representative datasets that would enable robust specialization, and (ii) restrictions on fine-tuning proprietary, state-of-the-art models (e.g., GPT-5.2 Max at the time of writing this paper)\cite{openai-restriction}. As a result, organizations are left with an uncomfortable choice between unadapted general-purpose models and costly, fragile prompt engineering or fine-tuning earlier models that are generally less capable---neither of which naturally produces the traceability and evidence expected in certification-oriented development.

To address these barriers, we introduce a novel neuro-symbolic \textbf{Spec-Code-Proof (SCP)} pipeline designed to bridge the gap between natural language requirements and verified implementation. Our approach integrates state-of-the-art Codex software engineering agents into the DARPA PROVERS \textbf{INSPECTA} (Industrial-Scale Proof Engineering for Critical Trustworthy Applications) toolchain~\cite{hardin2025dasc} as a user-friendly Spec-Code-
Proof (SCP) SysMl v2 Codex Copilot. INSPECTA provides the necessary formal rigor by leveraging \textbf{SysML v2}~\cite{introSysMLv2} for architectural definition and the \textbf{GUMBO} contract language~\cite{hatcliff2023gumbo} for behavioral specification. Crucially, the pipeline automates the translation of these models into executable, memory-safe code using the \textbf{HAMR} (High Assurance Modeling and Rapid engineering) framework~\cite{belt2022aeic, hatcliff2021hamr} and mathematically proves correctness using the \textbf{Logika} verifier~\cite{sireumlogika}, which utilizes SMT solvers such as Z3~\cite{moura2008z3} and CVC5~\cite{barbosa2022cvc5}.

\subsection{Theoretical Motivation: Meta-Rules as Persistent ICL}
A key innovation of our approach is the method by which we adapt the LLM to this complex toolchain without fine-tuning. We rely on a mechanism grounded in recent interpretability research. Recent theoretical work by von Oswald et al. mechanistically demonstrates that In-Context Learning (ICL) functions as a form of "implicit fine-tuning," where the model updates its internal representations via gradient descent dynamics during the forward pass~\cite{vonoswald2023transformers}. However, this learned capability is ephemeral; once the context window is reset, the knowledge is lost. 

The SCP Codex Copilot bridges this gap by \textit{crystallizing} high-quality ICL interactions—derived from golden examples and human feedback—into "Meta-Rules." We define Meta-Rules as reusable assets that transfer ephemeral ICL knowledge into persistent, verified formalization logic. This rules-based framework functions as a verifiable surrogate for black-box weight adaptation, effectively capturing the results of implicit optimization. This approach eliminates the need for repetitive, token-heavy context prompting while avoiding the proprietary and computational constraints of traditional parameter fine-tuning.

\subsection{Related Work}
Recently, generative AI, and particularly LLMs, have shown promising potential to improve explainability and guide automated formal verification. Early efforts include OpenAI’s GPT-f, which achieved notable success in Metamath theorem proving~\cite{polu2020generative, han2021generative}. Other initiatives have applied LLMs successfully to proof repair in Isabelle/HOL~\cite{first2023baldur}, theorem diagnosis in Coq~\cite{yang2023leandojo}, and discovering program invariants~\cite{garg2016learning, si2018learning}. Stanford and VMware’s Clover project represents another significant step forward, focusing on verifiable code generation with generative assistance~\cite{sun2023clover}. 

In the domain of conversational formal reasoning, Tahat et al. demonstrated high success rates using multi-turn conversational LLMs for proof repair in Coq, underscoring conversational learning’s value in formal reasoning domains~\cite{tahat2023coq}. However, purely neural approaches face limits; Apple’s GSM-Symbolic~\cite{gsm2024} highlighted fundamental limitations of LLMs in symbolic reasoning tasks. Similarly, Amazon’s recent SMT-backed hallucination prevention framework~\cite{amazon2024smt}, while innovative, remains closed-source, available exclusively as a web service, and has yet to integrate within aerospace-specific MBSE pipelines. Closer to our architectural focus, Tahat et al. introduced \textit{AGREE-Dog}~\cite{agreedog}, a neuro-symbolic copilot for the OSATE AADL environment that automates explainable compositional reasoning and repair, laying the groundwork for the SysML v2 approach presented in this paper.

\subsection{Contributions and Organization}
The main contributions of this paper are:
\begin{enumerate}
    \item \textbf{Trustworthy English-to-Implementation Pipeline:} A multi-agent framework integrated with the INSPECTA toolchain (HAMR/Logika) that generates verified contracts and code from requirements.
    \item \textbf{Meta-Rules Methodology:} A novel neuro-symbolic framework that functions as a verifiable surrogate for fine-tuning. By crystallizing ephemeral ICL into locally stored symbolic assets, the system iteratively adapts via formal verification feedback to drive a self-healing loop that repairs contracts and skeletal code until strict code-level verification succeeds.
    \item \textbf{Empirical Evaluation:} An evaluation on the Isolette benchmark~\cite{hatcliff2024isolette} (a proxy for avionics subsystems), demonstrating a 58$\times$ throughput increase and 100\% verification success compared to a failing baseline.
\end{enumerate}

The remainder of this paper is organized as follows. Section II details the System Architecture and the integration with INSPECTA. Section III presents the Meta-Rules methodology. Section IV describes the Isolette case study and experimental results. Section V discusses limitations and future work, and Section VI concludes the paper.
%--
\begin{lstlisting}[caption={background and related work },label={lst:rules}]
# TODO: to add background on sysmlv2/Gumbo contracts sec, and update bib for ref
\end{lstlisting}

\section{System Architecture}
The SCP Codex Copilot introduces a multi-agent "Copilot Loop" that integrates Large Language Models (LLMs) with the INSPECTA toolchain.

\subsection{Neuro-Symbolic Workflow}
The architecture operates as a coordinated multi-agent loop. As shown in Fig. \ref{fig:workflow}, the system parses English requirements, applies meta-rules for formalization, and generates model contracts. It then validates these contracts using the \textit{SysML v2 Sireum CLI} and \textit{HAMR/Logika} tools. If verification fails, a "Judge Agent" uses minimal human hints to repair the specification before generating the final verified system code~\cite{slides}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/copilot-loop.png}
\caption{SCP copilot self-healing neuro-symbolic code generation loop. It parses English requirements, applies meta-rules, and utilizes the INSPECTA toolchain for verification.}
\label{fig:workflow}
\end{figure}

\subsection{Meta-Rules Integration}
To overcome the limitations of pre-trained models, the platform uses "Meta-Rules"—transferable formalization rules learned through a supervised process. 

Instead of relying on messy fine-tuning of a huge model, the SCP platform uses a smart "teacher" agent to create these clear, reusable rules. As detailed in Algorithm~\ref{alg:meta-rules}, this supervised self-adaptation loop watches the main AI, spots patterns where it struggles (e.g., "hold previous value"), writes a generic mini-template (the meta-rule), and tests it before adding it to the rulebook.

Listing \ref{list:metaRules} illustrates a specific resulting rule for the "State" section. This rule directs the agent to introduce persistent variables when requirements imply memory, such as hysteresis or timeouts.

\begin{lstlisting}[caption={Extract from Gumbo FSE Agent Plan: Meta-Rules for State Formalization.}, label={list:metaRules}]
/* * 2) WHEN TO USE EACH SECTION
 * 2.1 state
 * Add if the English requires memory:
 * - Hysteresis "shall not be changed" / "hold previous" 
 * (e.g., Heat within desired band; Alarm no-change band).
 * - Timeouts (duration in mode > 1.0 s) if not directly 
 * available from the platform as a primitive.
 */

// Pattern examples provided to the Agent:
state
  lastHeat: Isolette_Data_Model::On_Off;
  initElapsed: Base_Types::S64; // time units
\end{lstlisting}

\subsection{Learning Meta-Rules from Examples}
It is crucial to distinguish between the two distinct stages of the Meta-Rules approach. For the end-user, Stage 1—applying existing meta-rules—is straightforward and transparent; the copilot simply uses the established plan. However, Stage 2—learning *new* rules—is a semi-automatic process that requires a supervised feedback loop and careful tuning of learning parameters by developers.

We have developed a technique to automate the creation of these rules, visualized in Fig. \ref{fig:ruleLearning}. It works by utilizing pairs of English requirements and their corresponding correct GUMBO formalization. The system analyzes these pairs to find common patterns and then generates a generalized 'meta-rule'. This rule can then be applied to new, unseen requirements.

The procedural steps for this semi-automatic learning process are outlined in Algorithm ~\ref{alg:meta-rules}.

\section{Demo Key Moments}
The demonstration utilized the "Isolette System" to benchmark the platform's capabilities against a direct formalization baseline.

\subsection{Moment 1: The Baseline Failure}
In the initial evaluation, a "direct formalization baseline" using Codex GPT-5.1 Max was tasked with translating requirements into GUMBO contracts. As detailed in the failure log (Listing \ref{list:failureLog}), the model failed to encode FAA requirements related to hysteresis and state variables.

\begin{lstlisting}[caption={Baseline Failure Log: Transcript from the failed session showing parser rejection and massive token usage.}, label={list:failureLog}, float=t]
- Baseline - Codex worked for 33m with virtually no success

I started encoding the remaining FAA requirements (hysteresis, "no change", etc.) directly in the SysML GUMBO clauses. To do so I tried to introduce state variables such as last_alarm_cmd and last_heat_cmd so that we could talk about previous actuator commands (which is necessary for REQ-MA-3 and REQ-MHS-4). However the SysML parser rejects every variant of the state ... syntax I've tried, and it also treats @pre as an unsupported "classification test" operator. In other words, the current toolchain doesn't appear to accept any of the GUMBO constructs that refer to component state or pre-state values, which makes it impossible to capture the remaining requirements faithfully.

Could you share an example (or enable access to the original AADL GUMBO subclause) that shows the specific syntax this environment expects for declaring state variables and referencing their pre/post values? Once I know the accepted form I can finish adding the REQ-MA-3/5 and REQ-MHS-4 cases.

Token usage: total=1,067,603 input=981,770 (+ 18,673,920 cached) output=85,833 (reasoning 54,080)
To continue this session, run codex resume 019ab4a9-d677-7631-9b23-aeda551e9513
\end{lstlisting}

The listing above illustrates the core failure mode. The model is running out of memory and context because it tried every syntactic combination it knew and failed. It is attempting to base its formalization on AADL concepts (like `@pre`), but it does not know how to translate those into the target SysML v2 GUMBO syntax. It lacks the self-confidence and knowledge to formalize the requirements correctly on its own.

Therefore, the solution is not vastly more compute, but smarter teaching. We must add reusable, transferable teaching artifacts. This approach is what we call "Meta-Rules."

\subsection{Moment 2: Success via Meta-Rules}
Upon applying the Meta-Rules (as shown in Listing \ref{list:metaRules}), the SCP Copilot successfully formalized all 40+ requirements. The listing below (Listing \ref{list:successLog}) displays the copilot's summary report after this successful run.

\begin{lstlisting}[caption={Success Log: Transcript showing successful formalization and verification after applying Meta-Rules.}, label={list:successLog}, float=t]
Codex optimized with meta-Rules. Worked for ~25 minutes with complete success

- Regulate.sysml:5-9 now imports Base_Types so the new GUMBO blocks can reference typed integer literals...
- Regulate.sysml:150-241 embeds the Manage Regulator Interface contract...
- Regulate.sysml:281-353 adds the Manage Regulator Mode contract with state for the last mode and INIT timer plus helper predicates that encode the Regulator Status definition from Table A-10; cases REQ_MRM_1-REQ_MRM_4 reproduce the diagrammed transitions and latch semantics.
- Regulate.sysml:394-476 introduces the Manage Heat Source contract: Integration assumes the desired-range/temperature bounds, initialization forces the heat off, state remembers the previous actuation for hysteresis, and compute cases cover REQ_MHS_1-REQ_MHS_5 plus the explicit 6s latency references...

Verification
- ./sireum hamr sysml logika --sourcepath isolette/sysml
- isolette/hamr/slang/bin/run-logika.cmd
Both steps completed successfully (only the customary "Ignoring ... connection" notices); Logika proved every initialise and timeTriggered method for Monitor and Regulate threads.

Token usage: total=369,318 input=317,485 (+ 4,302,976 cached) output=51,833 (reasoning 36,032)
\end{lstlisting}

Listing \ref{list:successLog} demonstrates that the issues identified in the failure log have been completely resolved. The summary confirms that the copilot has now "remember[ed] the previous actuation for hysteresis" and correctly encoded the "Manage Regulator Mode contract with state," proving that the Meta-Rules successfully taught the agent the correct syntax for state and memory. Consequently, the verification tools (HAMR/Logika) ran successfully, achieving 100\% proof coverage for both the Monitor and Regulate threads~\cite{slides}.

\subsection{Algorithm}
Algorithm~\ref{alg:meta-rules} presents Meta-Rule learning, application, and bounded repair with human supervision.

\subsubsection{Algorithm Description: Neuro-Symbolic Meta-Rule Learning and Application}
Algorithm~\ref{alg:meta-rules} formalizes the SCP Codex workflow as a dual-phase neuro-symbolic process designed to ensure the generation of trustworthy, formally verified system code. The algorithm distinguishes between the acquisition of transferable knowledge (Phase I) and its application to production tasks (Phase II), integrating formal verification tools directly into the learning loop to guarantee correctness.

\textbf{Phase I: Meta-Rule Extraction and Validation.}
The first phase focuses on the supervised acquisition of "Meta-Rules"—abstracted templates that map natural language patterns to formal GUMBO constructs. The process iterates over a set of "golden examples" ($G$), which consist of paired English requirements and their ground-truth GUMBO formalizations.
\begin{itemize}
    \item \textit{Extraction:} For each golden pair, the system derives a candidate meta-rule ($m$) via the \textsc{ExtractRule} function.
    \item \textit{Generalization Check:} To prevent overfitting, the candidate rule is tested against a sample subset of the target requirements ($R$). The system generates a candidate formalization ($\hat{g}$) and first applies a cosine distance check (using threshold $\epsilon$) to ensure structural conformity.
    \item \textit{Verification-Guided Repair:} The candidate formalization is submitted to the verifier ($V$), utilizing the HAMR/Logika toolchain. If verification fails, the system enters a bounded repair loop constrained by budget ($B$).
    \item \textit{Acceptance:} Only rules that successfully verify across the sample set—meeting the acceptance threshold ($\tau$)—are committed to the persistent Meta-Rules library ($M$).
\end{itemize}

\textbf{Phase II: Rule-Guided Formalization.}
The second phase represents the production deployment. Instead of relying on the stochastic generation of a raw LLM, the system utilizes the validated library ($M$) to synthesize formal contracts ($\hat{g}$).
\begin{itemize}
    \item \textit{Synthesis:} The \textsc{SynthesizeWithRules} function applies deterministic patterns, reducing hallucination.
    \item \textit{Self-Healing:} A similar verification-guided repair loop is employed to resolve semantic errors.
    \item \textit{Outcome:} Specifications that pass verification are added to set ($C$), while stubborn failures are isolated in set ($F$) for expert human review.
\end{itemize}

\begin{algorithm}[t]
\caption{SCP Meta-Rules Learning, Application, and Verification-Guided Repair}
\label{alg:meta-rules}
\KwIn{Requirements set $R$; golden examples $G$ (English + normalized GUMBO); verifier $V$ (HAMR/Logika); budgets $B$ (time/tokens/repair cycles); similarity threshold $\epsilon$; acceptance threshold $\tau$.}
\KwOut{Verified contracts $C$; retained Meta-Rules library $M$; flagged failures $F$.}

\textbf{Initialization:}\\
$M \leftarrow \emptyset$ \tcp*{persistent rule memory}
$C \leftarrow \emptyset$; $F \leftarrow \emptyset$\\

\BlankLine
\textbf{Phase I: Meta-Rule extraction and validation}\\
\ForEach{$(r_g, g_g) \in G$}{
  $m \leftarrow \textsc{ExtractRule}(r_g, g_g)$\;
  $okCount \leftarrow 0$\;
  \ForEach{$r \in \textsc{Sample}(R)$}{
    $\hat{g} \leftarrow \textsc{ApplyRule}(m, r)$\;
    \If{$\textsc{Distance}(\hat{g}, g_g) > \epsilon$}{\textbf{continue}\;}
    $(ok, fb) \leftarrow V(\hat{g}, B)$\;
    \While{\textnormal{not} $ok$ \textbf{and} \textsc{RepairBudgetRemaining}$(B)$}{
      $\hat{g} \leftarrow \textsc{Repair}(\hat{g}, fb, B)$\;
      $(ok, fb) \leftarrow V(\hat{g}, B)$\;
    }
    \If{$ok$}{$okCount \leftarrow okCount + 1$\;}
  }
  \eIf{$okCount \ge \tau$}{
    $M \leftarrow M \cup \{m\}$ \tcp*{retain only if it verifies and generalizes}
  }{
    \textsc{DiscardOrRefineWithHuman}$(m)$\;
  }
}

\BlankLine
\textbf{Phase II: Rule-guided formalization with bounded repair}\\
\ForEach{$r \in R$}{
  $\hat{g} \leftarrow \textsc{SynthesizeWithRules}(r, M)$\;
  $(ok, fb) \leftarrow V(\hat{g}, B)$\;
  \While{\textnormal{not} $ok$ \textbf{and} \textsc{RepairBudgetRemaining}$(B)$}{
    $\hat{g} \leftarrow \textsc{Repair}(\hat{g}, fb, B)$\;
    $(ok, fb) \leftarrow V(\hat{g}, B)$\;
  }
  \eIf{$ok$}{
    $C \leftarrow C \cup \{(r,\hat{g})\}$\;
  }{
    $F \leftarrow F \cup \{(r,\hat{g},fb)\}$\;
  }
}

\BlankLine
\textbf{Quality assessment and logging:}\\
Log timestamps, token usage, latency, and repair count; store $M$ for reuse.

\end{algorithm}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/rule-learning.png}
\caption{The semi-automatic supervised learning loop used to generate transferable Meta-Rules from requirement/GUMBO pairs.}
\label{fig:ruleLearning}
\end{figure}

\subsubsection{Discussion: Human-in-the-Loop Refinement}
While the Meta-Rule extraction process is highly automated, the "Human Expert" remains essential to ensuring the robust transferability of knowledge. The learning process begins with high-quality inputs; beyond standard requirement text, human experts provide domain-specific documentation—such as the Steve Miller illustrations~\cite{}—which serve as rich, semantic anchors.

A critical aspect of the framework is the \textbf{Generalization-Specialization Trade-off}, managed through a hybrid of mechanical metrics and human review:
\begin{itemize}
    \item \textbf{Semantic Proximity:} The system employs a cosine distance metric to evaluate the semantic similarity between the generated contract and the "Golden" reference. Counter-intuitively, a distance of zero may signal overfitting (the rule is too specific). In such cases, the expert generalizes the rule structure to capture intent rather than instance.
    \item \textbf{Human-Guided Refinement:} If a candidate rule is too general, it risks hallucinations on corner cases; the expert provides specific examples to constrain it. Conversely, if a rule is overly rigid, the expert modifies the template to broaden applicability.
\end{itemize}

\section{Evaluation and Metrics}
The quantitative impact of the Meta-Rules is visualized in Fig. \ref{fig:resultsChart}. 

\begin{lstlisting}[caption={Meta-Rules },label={lst:rules}]
# TODO: to add metrics definitions subsection.
\end{lstlisting}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figures/results-chart.png}
\caption{Performance comparison: Before (Orange) vs. After Meta-Rules (Blue). The charts demonstrate a massive reduction in input/output tokens and execution time, while Task Throughput increases by orders of magnitude.}
\label{fig:resultsChart}
\end{figure}

\subsection{Performance Comparison}
As illustrated in Fig. \ref{fig:resultsChart}, the orange bars represent the failing baseline, while the blue bars represent the Fine-Tuned Meta-Rules approach.
\begin{itemize}
    \item \textbf{Token Reduction:} The baseline (Orange) consumed $\approx$19M input tokens. After applying Meta-Rules (Blue), input tokens dropped to $\approx$4.9M (cached) and only $\approx$200k uncashed. 
    \item \textbf{Reasoning Efficiency:} Output and reasoning tokens saw significant reductions, dropping from $>80$k to $\approx$51k output tokens.
    \item \textbf{Execution Time:} The total time was cut from 33 minutes to $\approx$25 minutes, transforming a failing workflow into a fully verified one.
    \item \textbf{Correctness:} The approach achieved a $>58\times$ improvement in correctness, moving from near-zero to 100\% verification success~\cite{slides}.
\end{itemize}

\begin{lstlisting}[caption={Meta-Rules },label={lst:rules}]
# TODO: Limitations and future work section.
\end{lstlisting}

\section{Conclusion}
The SCP Codex Open Platform demonstrates that using Meta-Rules to extract and reuse formalization knowledge significantly outperforms direct model querying. This neuro-symbolic approach enables the reliable generation of trustworthy, formally verified system code with orders-of-magnitude improvements in efficiency and cost.

% MANUALLY DEFINED BIBLIOGRAPHY FOR INSTANT COMPILATION
%\begin{thebibliography}{1}

%\bibitem{slides}
%A.~Tahat, D.~Hardin, I.~Amundson, and D.~Cofer, ``Neurosymbolic Automated Contract and Code
%Generation for SysML v2 Models'' Collins Aerospace \& DARPA PROVERS, Jan. 2026.

%\bibitem{hardin2025dasc}
%D.~Hardin et al., ``INSPECTA: Industrial-Scale Proof Engineering for Critical Trustworthy Applications,'' \textit{Digital Avionics Systems Conference (DASC)}, 2025.

%\end{thebibliography}

%====================================================================
\bibliographystyle{IEEEtran}
\bibliography{biblio}


%===========
\appendix
\section{Meta-Rules File}
\label{app:rules}
\todo{Insert a link to the complete Meta-Rules file here.}

\begin{lstlisting}[caption={Meta-Rules },label={lst:rules}]
# TODO: Paste complete rules file here.
\end{lstlisting}

\end{document}