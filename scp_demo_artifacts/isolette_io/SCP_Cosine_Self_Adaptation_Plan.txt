
# SCP_Cosine_Self_Adaptation_Plan – Cosine Distance Mode

## Purpose

1- The goal of this plan is to iteratively adapt the rules in FSE plan so that the rules generate GUMBO contrcts generated into 
Isolette model do converge toward the golden reference text using a
semantic similarity metric, Verification conditions, and human approvals.  We use GPT‑embedding **cosine distance** as the first
metric: a smaller cosine distance indicates a candidate contract is semantically
close to the golden specification not too close and not too far. Changes on meta-rules are accepted block by block
if they produce a cosine distance bounded by a small predfined epsilon (we aim to maximaize) and all
verification checks pass and ask human for review and approval. Only met-rules that leads to improvements are permitted; no regressions are
allowed. (if a change on a rule failed any acceptance criteria it is rejected and wont be written in the Rules book). 

## Environment & Assumptions

- Full network access is available.
- An OpenAI API key is present (e.g., `OPENAI_API_KEY`).
- The default embedding model is `gpt‑text‑embedding‑small`.
- Shell tooling required for Logika verification is available, e.g.:
  - For integration level:. 
    - `sireum hamr sysml logika --sourcepath .` OR
    - `sireum hamr sysml logika --sourcepath isolette/sysml`
  - For code-level:  
    - `isolette/hamr/slang/bin/run-logika.cmd`.
- Files and directories:
  - Current model: `isolette/sysml/Monitor.sysml`.  (or as specified by user)
  - Golden model:  `golden_examples/isolette/sysml/Monitor.sysml`. (or as specifed by the user)
  - Progress log: `progress.json`. (locate it in a subdirectory of the current directory and call it Gumbo_FSE_plans_progress_json_logs/.) 
  - Plan checkpoints directory: `Gumbo_FSE_plan_progress/`.
  - Per‑contract overrides: `contract_overrides.json`.

## Tracked Contracts

The Gumbo blocks in the target sysml file are tracked, for example:

The following GUMBO blocks in `Monitor.sysml` are tracked and compared against
their golden counterparts.  If a block is absent in the current model, this is
noted in the log and the plan moves to the next block.

- `part def Manage_Monitor_Interface_i`
- `part def Manage_Monitor_Mode_i`
- `part def Manage_Alarm_i`

## Metrics Recording

To evaluate the effectiveness of this plan and to enable later analysis, we
record detailed metrics before and after adaptation for each contract.

### Baseline (Pre‑Plan) Recording

Before applying any plan modifications, run Logika verification on each
contract’s current implementation and record the following metrics:

| metric | description |
|---|---|
| `baseline_tokens` | Total tokens consumed during verification (input + output + reasoning).  Reasoning tokens are counted separately from input/output tokens to reflect their additional computational cost. |
| `baseline_reasoning_tokens` | Tokens generated by the model’s internal reasoning process.  These are recorded separately because reasoning tokens are billed at a higher ratehttps://journal.uptimeinstitute.com/reasoning-will-increase-the-infrastructure-footprint-of-ai/#:~:text=There%20are%20several%20approaches%20to,treats%20them%20as%20output%20tokens. |
| `baseline_runtime` | Wall‑clock time (seconds) spent verifying the contract with Logika. |
| `assume_count` and `guarantee_count` | Number of `assume` and `guarantee` statements within the block. |

These baseline values are stored in `progress.json` under a `baseline` entry for
the corresponding contract.

### Post‑Plan Recording

After completing the iterative adaptation loop and final verification succeeds,
record the same set of metrics for the final candidate text:

| metric | description |
|---|---|
| `final_tokens` | Total tokens consumed during verification of the final version. |
| `final_reasoning_tokens` | Reasoning tokens consumed by the final verification. |
| `final_runtime` | Wall‑clock verification time for the final version. |

These final values are stored under the plan version entry in `progress.json`.

### Metric Aggregation and Reporting

Once baseline and final metrics have been recorded, compute the following per
plan:

1. **Token savings per block:** `tokens_saved = baseline_tokens – final_tokens`.
2. **Runtime savings per block:** `runtime_saved = baseline_runtime – final_runtime`.
3. **Average and median** baseline and final token counts and run times across all
   tracked contracts.  The median is included because it is less sensitive to
   outliers than the averagehttps://www.catchpoint.com/blog/mean-performance-analysis#:~:text=If%20we%20use%20median%20it,center%20of%20the%20Gaussian%20distribution.
4. **Efficiency metrics:**
   - `contracts_per_minute = (# verified contracts) / (sum(final_runtime)/60)`.
   - `contracts_per_1k_tokens = (# verified contracts) / (sum(final_tokens) / 1000)`.
5. **Human interventions:** Count the number of manual edits or approvals needed
   for each plan execution.

These aggregate metrics are written to `progress.json` and later consumed by a
visualisation script.

## Baseline Measurement

For each tracked contract, extract the GUMBO block text from the current model
and its golden reference.  Compute the cosine distance between the current text
and the golden text using the selected embedding model.  Record the distance as
the baseline metric for that contract along with the baseline tokens and
runtime.

## Acceptance Rule (Cosine Distance)

During adaptation, synthesise a candidate text for each contract using the
current plan rules.  Compute the cosine distance between the candidate and the
golden reference.  Accept the candidate if:

```
candidate_distance <= previous_distance × (1 – epsilon)
```

where `epsilon` is a small value (default 0.001) to ensure meaningful
improvements.  Only accept candidates that also pass all verification steps.
Non‑improving or verification‑failing candidates are discarded in favour of
the last accepted version.

## Iterative Adaptation Loop

1. **Synthesis:** Generate candidate texts for all tracked contracts using
   `Gumbo_FSE_cosine_Plan.md` rules.
2. **Metric evaluation:** For each candidate, compute the cosine distance and
   record the candidate metric.
3. **Acceptance:** Apply the acceptance rule; accept candidates that
   sufficiently reduce the distance.  Others retain the last accepted text.
4. **Hybrid assembly:** Assemble a hybrid model with all accepted contracts and
   run Logika verification (model‑level then code‑level).
5. **Greedy salvage:** If verification fails, attempt to salvage the best
   improvements by adding them one by one from the largest distance reduction
   downward until verification succeeds.
6. **Checkpoint:** If at least one contract improved and the hybrid verifies,
   write the new texts, record metrics, and update `contract_overrides.json`.
7. **Termination:** Stop when either no contracts improve in a user defined unit of time max 10 minutes or the user requests
   termination.

## Verification Workflow

Follow the same sequence as in the original plan: run integration Logika
(`sireum hamr sysml logika --sourcepath isolette/sysml`), then run code‑level
Logika (`isolette/hamr/slang/bin/run-logika.cmd`).  On failure, document the
errors, suggest a repair plan, and require human approval before proceeding.

## Reporting & Termination

After each iteration, append an entry to `progress.json` summarising:

- Plan version and timestamp.
- Mode (`cosine-dist`).
- Embedding model used and epsilon.
- Per‑contract baseline and final metrics (distances, token counts, run times,
  savings, assume/guarantee counts).
- Aggregate metrics (averages, medians, efficiency, human interventions).
- Verification status and runtime.
- Which contracts improved and whether they converged (the converged is defined by the passing the distance condition and the two verificaiton conditions and the final human approval to write the plan check point).

## Visualisation and Summary

After completing adaptation and logging, run the script `visualize_metrics.py` in tools/.  (if not in current directory find in ./tools)
This script reads `progress.json` whereever they are located (default search is in the current dir or Gumbo_FSE_plans_progress_json_logs/. ) and produces several plots:

1. **Arrow scatter plot** showing tokens vs. run time for each contract,
   with arrows from baseline to final values.
2. **Token footprint bar chart** comparing average baseline tokens to average
   final tokens.
3. **Savings per contract** bar chart showing tokens and runtime saved.
4. **Efficiency bar charts** for contracts verified per minute and per 1 k
   tokens.
5. **Human intervention bars** summarising manual effort per plan.

The plots are saved as images and can be embedded in a summary report (e.g.,
`metrics_summary.md`).
